{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8056a416-e67c-47fe-b98b-61a2ca7ef081",
   "metadata": {},
   "source": [
    "# Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c28cd6-c0eb-4a1b-a03d-137da67de586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from PIL import ImageColor\n",
    "\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2efd958-551f-452a-9935-5e4dd00c5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, raw_point):\n",
    "        self.x = raw_point[0]\n",
    "        self.y = raw_point[1]\n",
    "\n",
    "        def to_string(self): '(' + str(self.x) + ', ' + str(self.y) + ')'\n",
    "\n",
    "        def to_dict(self): return {'x': self.x, 'y': self.y}\n",
    "\n",
    "\n",
    "class Box:\n",
    "    def __init__(self, class_name, confidence, raw_corner_points, color, track_id=None):\n",
    "        self.class_name = class_name\n",
    "        self.confidence = confidence\n",
    "        self.raw_corner_points = raw_corner_points\n",
    "        self.top_left_point = Point(raw_corner_points[0])\n",
    "        self.bottom_right_point = Point(raw_corner_points[1])\n",
    "        self.width = self.bottom_right_point.x - self.top_left_point.x\n",
    "        self.height = self.bottom_right_point.y - self.top_left_point.y\n",
    "        self.color = color\n",
    "        self.track_id = track_id\n",
    "\n",
    "    def to_dict(self):\n",
    "        box = OrderedDict([\n",
    "            ('class', self.class_name),\n",
    "            ('confidence', self.confidence),\n",
    "            ('x', self.top_left_point.x),\n",
    "            ('y', self.top_left_point.y),\n",
    "            ('width', self.width),\n",
    "            ('height', self.height),\n",
    "            ('color', self.color)\n",
    "        ])\n",
    "        if self.track_id is not None:\n",
    "            box['id'] = self.track_id\n",
    "        return box\n",
    "\n",
    "\n",
    "class Detections:\n",
    "    def __init__(self, raw_detection, classes, tracking=False):\n",
    "        self.__raw_detection = raw_detection\n",
    "        self.__classes = classes\n",
    "        self.__boxes = []\n",
    "        self.__tracking = tracking\n",
    "        self.__point1_index = 0\n",
    "        self.__point2_index = 1\n",
    "        self.__point3_index = 2\n",
    "        self.__point4_index = 3\n",
    "        self.__tracking_index = 4\n",
    "        self.__class_index = 5 if tracking else 5\n",
    "        self.__confidence_index = 6 if tracking else 4\n",
    "        self.__extract_boxes()\n",
    "\n",
    "    def __extract_boxes(self):\n",
    "        for raw_box in self.__raw_detection:\n",
    "            track_id = None\n",
    "            if self.__tracking:\n",
    "                track_id = int(raw_box[self.__tracking_index])\n",
    "            class_id = int(raw_box[self.__class_index])\n",
    "            raw_corner_points = (int(raw_box[self.__point1_index]), int(raw_box[self.__point2_index])), (\n",
    "                int(raw_box[self.__point3_index]), int(raw_box[self.__point4_index]))\n",
    "            confidence = raw_box[self.__confidence_index]\n",
    "            dataset_class = self.__classes[class_id]\n",
    "            class_name = dataset_class['name']\n",
    "            class_color = dataset_class['color']\n",
    "            box = Box(class_name, confidence, raw_corner_points, class_color, track_id=track_id)\n",
    "            self.__boxes.append(box)\n",
    "\n",
    "    def get_boxes(self):\n",
    "        return self.__boxes\n",
    "\n",
    "    def to_dict(self):\n",
    "        boxes = []\n",
    "        for box in self.__boxes:\n",
    "            boxes.append(box.to_dict())\n",
    "        return boxes\n",
    "\n",
    "    def to_json(self):\n",
    "        boxes = self.to_dict()\n",
    "        return json.dumps(boxes, indent=4)\n",
    "\n",
    "\n",
    "def plot_box(image, top_left_point, bottom_right_point, width, height, label, num_people, color=(210, 240, 0), padding=6,\n",
    "             font_scale=0.35):\n",
    "    label = label.upper()\n",
    "\n",
    "    cv2.rectangle(image, (top_left_point['x'] - 1, top_left_point['y']),\n",
    "                  (bottom_right_point['x'], bottom_right_point['y']), color, thickness=2, lineType=cv2.LINE_AA)\n",
    "    res_scale = (image.shape[0] + image.shape[1]) / 1600\n",
    "    font_scale = font_scale * res_scale\n",
    "    font_width, font_height = 0, 0\n",
    "    font_face = cv2.FONT_HERSHEY_DUPLEX\n",
    "    text_size = cv2.getTextSize(label, font_face, fontScale=font_scale, thickness=1)[0]\n",
    "\n",
    "    if text_size[0] > font_width:\n",
    "        font_width = text_size[0]\n",
    "    if text_size[1] > font_height:\n",
    "        font_height = text_size[1]\n",
    "    if top_left_point['x'] - 1 < 0:\n",
    "        top_left_point['x'] = 1\n",
    "    if top_left_point['x'] + font_width + padding * 2 > image.shape[1]:\n",
    "        top_left_point['x'] = image.shape[1] - font_width - padding * 2\n",
    "    if top_left_point['y'] - font_height - padding * 2 < 0:\n",
    "        top_left_point['y'] = font_height + padding * 2\n",
    "\n",
    "    p3 = top_left_point['x'] + font_width + padding * 2, top_left_point['y'] - font_height - padding * 2\n",
    "    cv2.rectangle(image, (top_left_point['x'] - 2, top_left_point['y']), p3, color, -1, lineType=cv2.LINE_AA)\n",
    "    x = top_left_point['x'] + padding\n",
    "    y = top_left_point['y'] - padding\n",
    "    cv2.putText(image, label, (x, y), font_face, font_scale, [0, 0, 0], thickness=1, lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def draw(image, detections):\n",
    "    image_copy = image.copy()\n",
    "    for box in detections:\n",
    "        class_name = box['class']\n",
    "        conf = box['confidence']\n",
    "        text = ''\n",
    "        if 'text' in box:\n",
    "            text = box['text']\n",
    "            if len(text) > 50:\n",
    "                text = text[:50] + ' ...'\n",
    "        label = (str(box['id']) + '. ' if 'id' in box else '') + class_name + ' ' + str(int(conf * 100)) + '%' + (\n",
    "            (' | ' + text) if ('text' in box and len(box['text']) > 0 and not box['text'].isspace()) else '')\n",
    "        width = box['width']\n",
    "        height = box['height']\n",
    "        color = box['color']\n",
    "\n",
    "        if isinstance(color, str):\n",
    "            color = ImageColor.getrgb(color)\n",
    "            color = (color[2], color[1], color[0])\n",
    "\n",
    "        top_left_point = {'x': box['x'], 'y': box['y']}\n",
    "        bottom_right_point = {'x': box['x'] + width, 'y': box['y'] + height}\n",
    "        image_copy = plot_box(image_copy, top_left_point, bottom_right_point, width, height, label, color=color)\n",
    "    return image_copy\n",
    "\n",
    "def counter(image, detections):\n",
    "    num_people = 0\n",
    "    for box in detections:\n",
    "        if box['class'] == 'person':\n",
    "            num_people += 1\n",
    "\n",
    "    cv2.putText(image, str(num_people), (50, 75), fontScale=2, fontFace=cv2.FONT_HERSHEY_DUPLEX, color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162a0c0-3239-4655-9de0-3506116f0e42",
   "metadata": {},
   "source": [
    "# Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b07a9d9-ed44-4118-adfa-d16224ad566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights='yolov7/yolov7.pt', classes='track_classes.yaml', device='cpu', video=None, output=None)\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Error Opening Video!\n",
      "Tracking Objects...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 frames [00:00, ? frames/s]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detections import draw\n",
    "from obj_detector import YOLOv7\n",
    "\n",
    "sys.path.insert(0, './yolov7')\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "\n",
    "def track_objects(weights, classes, device, video, output):\n",
    "    yolov7 = YOLOv7()\n",
    "    yolov7.load(weights_path=weights, classes=classes, device=device)\n",
    "\n",
    "    vid = cv2.VideoCapture(video)\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    frames_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    output = cv2.VideoWriter(output, fourcc, fps, (width, height))\n",
    "\n",
    "    if vid.isOpened() == False:\n",
    "        print('Error Opening Video!')\n",
    "\n",
    "    print('Tracking Objects...\\n')\n",
    "    pbar = tqdm(total=frames_count, unit=' frames', dynamic_ncols=True, position=0, leave=True)\n",
    "\n",
    "    try:\n",
    "        while vid.isOpened():\n",
    "            ret, frame = vid.read()\n",
    "            if ret:\n",
    "                detections = yolov7.detect(frame, track=True)\n",
    "                detected_frame = draw(frame, detections)\n",
    "                detected_frame = counter(detected_frame, detections)\n",
    "                output.write(detected_frame)\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    pbar.close()\n",
    "    vid.release()\n",
    "    output.release()\n",
    "    yolov7.unload()\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = os.path.join(os.getcwd(), 'yolov7/yolov7.pt')\n",
    "CLASSES_PATH = os.path.join(os.getcwd(), 'track_classes.yaml')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(prog='track_objects.py')\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default='yolov7/yolov7.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--classes', type=str, default='track_classes.yaml',\n",
    "                        help='YAML File of Object(s) to Track (See '\n",
    "                             'Documentation)')\n",
    "    parser.add_argument('--device', type=str, default='cpu', help='CPU or GPU')\n",
    "    parser.add_argument('--video', type=str, help='Video Path')\n",
    "    parser.add_argument('--output', type=str, help='(output_video_name).mp4')\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    track_objects(opt.weights, opt.classes, opt.device, opt.video, opt.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09662744-a8ff-45ce-8ef2-6035cb2f4fe9",
   "metadata": {},
   "source": [
    "Note that in the cell block above, the block is executed by cleaning `sys.argv` by evaluating \n",
    "`import sys; sys.argv=['']; del sys`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d697d-32af-4c4b-87f7-42f90987b427",
   "metadata": {},
   "source": [
    "For single frame inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd4a7a5-142f-44e3-81eb-3c7cdde4a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights='yolov7/yolov7.pt', classes='track_classes.yaml', device='cpu', image_file='test_frame.png', output='detected_test_frame.png')\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Number of People in Frame: 1\n",
      "[OrderedDict([('class', 'person'), ('confidence', 0.63), ('x', 0), ('y', 670), ('width', 63), ('height', 132), ('color', '#00ffc3'), ('id', 17)]), OrderedDict([('class', 'car'), ('confidence', 0.94), ('x', 618), ('y', 467), ('width', 250), ('height', 179), ('color', '#00ffc3'), ('id', 18)])]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "\n",
    "from detections import draw\n",
    "from obj_detector import YOLOv7\n",
    "\n",
    "sys.path.insert(0, './yolov7')\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "\n",
    "def track_single_frame(weights, classes, device, image_file, output):\n",
    "    yolov7 = YOLOv7()\n",
    "    yolov7.load(weights_path=weights, classes=classes, device=device)\n",
    "\n",
    "    image = cv2.imread(image_file)\n",
    "    detections = yolov7.detect(image, track=True)\n",
    "    detected_frame = draw(image, detections)\n",
    "    detected_frame = counter(detected_frame, detections)\n",
    "\n",
    "    num_people = 0\n",
    "    for detected_obj in detections:\n",
    "        if detected_obj['class'] == 'person':\n",
    "            num_people += 1\n",
    "\n",
    "    print(f'Number of People in Frame: {num_people}')\n",
    "\n",
    "    cv2.imwrite(output, detected_frame)\n",
    "\n",
    "    print(detections)\n",
    "    yolov7.unload()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(prog='track_single_frame.py')\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default='yolov7/yolov7.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--classes', type=str, default='track_classes.yaml',\n",
    "                        help='YAML File of Object(s) to Track (See '\n",
    "                             'Documentation)')\n",
    "    parser.add_argument('--device', type=str, default='cpu', help='CPU or GPU')\n",
    "    parser.add_argument('--image_file', type=str, default='test_frame.png', help='Image File Path')\n",
    "    parser.add_argument('--output', type=str, default='detected_test_frame.png', help='(output_image_name).png')\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    track_single_frame(opt.weights, opt.classes, opt.device, opt.image_file, opt.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef126e-2745-489e-af7e-8ffefd817f2a",
   "metadata": {},
   "source": [
    "We now have three pythons scripts (four including the main object detector class) that can be used via command-line interfaces to perform object tracking on videos with the script being easily modifiable to switch to a live-video feed for analytics. For more single frame inferences, the `track_single_frame.py` script can be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2f733-6220-489d-816f-19efdac84fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Capstone Project)",
   "language": "python",
   "name": "capstone-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
