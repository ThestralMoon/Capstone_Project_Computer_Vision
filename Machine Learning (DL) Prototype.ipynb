{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83893e22-b32f-446a-9d67-b3be33fc375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23366979-3f6d-4d14-8f02-7dfb4944eb6d",
   "metadata": {},
   "source": [
    "# Installing YOLOv7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e30cf0f-91d9-4291-ba96-777d7107afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Already Exists!\n"
     ]
    }
   ],
   "source": [
    "if not (os.path.exists(f'{HOME_DIR}/yolov7')):\n",
    "    !git clone https://github.com/WongKinYiu/yolov7.git\n",
    "    %cd yolov7\n",
    "    !pip install -r requirements.txt\n",
    "else:\n",
    "    print(\"Directory Already Exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3bc2f-0c98-4c33-bf6a-6ddaac47c2d4",
   "metadata": {},
   "source": [
    "# Installing ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837d8d9c-d3fb-4c09-a4f9-d64c689a4520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Already Exists!\n"
     ]
    }
   ],
   "source": [
    "if not (os.path.exists(f'{HOME_DIR}/ByteTrack')):\n",
    "    !git clone https://github.com/ifzhang/ByteTrack.git\n",
    "    os.chdir(f\"{HOME_DIR}/ByteTrack\")\n",
    "    !pip3 install -r requirements.txt\n",
    "    !python setup.py develop\n",
    "    !pip3 install cython\n",
    "    !pip install libpython\n",
    "    !pip install -e git+https://github.com/samson-wang/cython_bbox.git#egg=cython-bbox\n",
    "else:\n",
    "    print(\"Directory Already Exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a132eb-c248-4a05-ab9c-0dd32d0a180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f\"{HOME_DIR}/ByteTrack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8a992-3e95-4563-8d27-2822eda6f61b",
   "metadata": {},
   "source": [
    "We have to modify the byte_tracker.py file to match the detection format of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4466ab-87d8-4ee2-815a-b8bc9bddf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaheer\\anaconda3\\envs\\capstone_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %load ByteTrack/yolox/tracker/byte_tracker.py\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import os.path as osp\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from yolov7.utils.general import xywh2xyxy, xyxy2xywh\n",
    "from yolox.tracker.kalman_filter import KalmanFilter\n",
    "from yolox.tracker import matching\n",
    "from ByteTrack.yolox.tracker.basetrack import BaseTrack, TrackState\n",
    "\n",
    "class STrack(BaseTrack):\n",
    "    shared_kalman = KalmanFilter()\n",
    "    def __init__(self, tlwh, score, cls):\n",
    "        self._tlwh = np.asarray(tlwh, dtype=np.float)\n",
    "        self.kalman_filter = None\n",
    "        self.mean, self.covariance = None, None\n",
    "        self.is_activated = False\n",
    "        self.score = score\n",
    "        self.tracklet_len = 0\n",
    "        self.cls = cls\n",
    "\n",
    "    def predict(self):\n",
    "        mean_state = self.mean.copy()\n",
    "        if self.state != TrackState.Tracked:\n",
    "            mean_state[7] = 0\n",
    "        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_predict(stracks):\n",
    "        if len(stracks) > 0:\n",
    "            multi_mean = np.asarray([st.mean.copy() for st in stracks])\n",
    "            multi_covariance = np.asarray([st.covariance for st in stracks])\n",
    "            for i, st in enumerate(stracks):\n",
    "                if st.state != TrackState.Tracked:\n",
    "                    multi_mean[i][7] = 0\n",
    "            multi_mean, multi_covariance = STrack.shared_kalman.multi_predict(multi_mean, multi_covariance)\n",
    "            for i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\n",
    "                stracks[i].mean = mean\n",
    "                stracks[i].covariance = cov\n",
    "\n",
    "    def activate(self, kalman_filter, frame_id):\n",
    "        \"\"\"Start a new tracklet\"\"\"\n",
    "        self.kalman_filter = kalman_filter\n",
    "        self.track_id = self.next_id()\n",
    "        self.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n",
    "\n",
    "        self.tracklet_len = 0\n",
    "        self.state = TrackState.Tracked\n",
    "        if frame_id == 1:\n",
    "            self.is_activated = True\n",
    "        # self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        self.start_frame = frame_id\n",
    "\n",
    "    def re_activate(self, new_track, frame_id, new_id=False):\n",
    "        self.mean, self.covariance = self.kalman_filter.update(\n",
    "            self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n",
    "        )\n",
    "        self.tracklet_len = 0\n",
    "        self.state = TrackState.Tracked\n",
    "        self.is_activated = True\n",
    "        self.frame_id = frame_id\n",
    "        if new_id:\n",
    "            self.track_id = self.next_id()\n",
    "        self.score = new_track.score\n",
    "\n",
    "    def update(self, new_track, frame_id):\n",
    "        \"\"\"\n",
    "        Update a matched track\n",
    "        :type new_track: STrack\n",
    "        :type frame_id: int\n",
    "        :type update_feature: bool\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.frame_id = frame_id\n",
    "        self.tracklet_len += 1\n",
    "\n",
    "        new_tlwh = new_track.tlwh\n",
    "        self.mean, self.covariance = self.kalman_filter.update(\n",
    "            self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\n",
    "        self.state = TrackState.Tracked\n",
    "        self.is_activated = True\n",
    "\n",
    "        self.score = new_track.score\n",
    "\n",
    "    @property\n",
    "    # @jit(nopython=True)\n",
    "    def tlwh(self):\n",
    "        \"\"\"Get current position in bounding box format `(top left x, top left y,\n",
    "                width, height)`.\n",
    "        \"\"\"\n",
    "        if self.mean is None:\n",
    "            return self._tlwh.copy()\n",
    "        ret = self.mean[:4].copy()\n",
    "        ret[2] *= ret[3]\n",
    "        ret[:2] -= ret[2:] / 2\n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    # @jit(nopython=True)\n",
    "    def tlbr(self):\n",
    "        \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n",
    "        `(top left, bottom right)`.\n",
    "        \"\"\"\n",
    "        ret = self.tlwh.copy()\n",
    "        ret[2:] += ret[:2]\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    # @jit(nopython=True)\n",
    "    def tlwh_to_xyah(tlwh):\n",
    "        \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n",
    "        height)`, where the aspect ratio is `width / height`.\n",
    "        \"\"\"\n",
    "        ret = np.asarray(tlwh).copy()\n",
    "        ret[:2] += ret[2:] / 2\n",
    "        ret[2] /= ret[3]\n",
    "        return ret\n",
    "\n",
    "    def to_xyah(self):\n",
    "        return self.tlwh_to_xyah(self.tlwh)\n",
    "\n",
    "    @staticmethod\n",
    "    # @jit(nopython=True)\n",
    "    def tlbr_to_tlwh(tlbr):\n",
    "        ret = np.asarray(tlbr).copy()\n",
    "        ret[2:] -= ret[:2]\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    # @jit(nopython=True)\n",
    "    def tlwh_to_tlbr(tlwh):\n",
    "        ret = np.asarray(tlwh).copy()\n",
    "        ret[2:] += ret[:2]\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OT_{}_({}-{})'.format(self.track_id, self.start_frame, self.end_frame)\n",
    "\n",
    "\n",
    "class BYTETracker(object):\n",
    "    def __init__(self, args, frame_rate=30):\n",
    "        self.tracked_stracks = []  # type: list[STrack]\n",
    "        self.lost_stracks = []  # type: list[STrack]\n",
    "        self.removed_stracks = []  # type: list[STrack]\n",
    "\n",
    "        self.frame_id = 0\n",
    "        self.args = args\n",
    "        #self.det_thresh = args.track_thresh\n",
    "        self.det_thresh = args.track_thresh + 0.1\n",
    "        self.buffer_size = int(frame_rate / 30.0 * args.track_buffer)\n",
    "        self.max_time_lost = self.buffer_size\n",
    "        self.kalman_filter = KalmanFilter()\n",
    "\n",
    "    def update(self, dets):\n",
    "        self.frame_id += 1\n",
    "        activated_starcks = []\n",
    "        refind_stracks = []\n",
    "        lost_stracks = []\n",
    "        removed_stracks = []\n",
    "\n",
    "        xyxys = dets[:, 0:4]\n",
    "        xywh = xyxy2xywh(xyxys)\n",
    "        confs = dets[:, 4]\n",
    "        clss = dets[:, 5]\n",
    "\n",
    "        classes = clss\n",
    "        xyxys = xyxys\n",
    "        confs = confs\n",
    "\n",
    "        remain_inds = confs > self.args.track_thresh\n",
    "        inds_low = confs > 0.1\n",
    "        inds_high = confs < self.args.track_thresh\n",
    "\n",
    "        inds_second = np.logical_and(inds_low, inds_high)\n",
    "\n",
    "        dets_second = xywh[inds_second]\n",
    "        dets = xywh[remain_inds]\n",
    "\n",
    "        scores_keep = confs[remain_inds]\n",
    "        scores_second = confs[inds_second]\n",
    "\n",
    "        clss_keep = classes[remain_inds]\n",
    "        clss_second = classes[remain_inds]\n",
    "\n",
    "        if len(dets) > 0:\n",
    "            detections = [STrack(xyxy, s, c) for (xyxy, s, c) in zip(dets, scores_keep, clss_keep)]\n",
    "        else:\n",
    "            detections = []\n",
    "\n",
    "        unconfirmed = []\n",
    "        tracked_stracks = []\n",
    "        for track in self.tracked_stracks:\n",
    "            if not track.is_activated:\n",
    "                unconfirmed.append(track)\n",
    "            else:\n",
    "                tracked_stracks.append(track)\n",
    "\n",
    "        strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n",
    "        STrack.multi_predict(strack_pool)\n",
    "        dists = matching.iou_distance(strack_pool, detections)\n",
    "        dists = matching.fuse_score(dists, detections)\n",
    "        matches, u_track, u_detection = matching.linear_assignment(dists, thresh=self.args.match_thresh)\n",
    "\n",
    "        for itracked, idet in matches:\n",
    "            track = strack_pool[itracked]\n",
    "            det = detections[idet]\n",
    "            if track.state == TrackState.Tracked:\n",
    "                track.update(detections[idet], self.frame_id)\n",
    "                activated_starcks.append(track)\n",
    "            else:\n",
    "                track.re_activate(det, self.frame_id, new_id=False)\n",
    "                refind_stracks.append(track)\n",
    "\n",
    "        if len(dets_second) > 0:\n",
    "            detections_second = [STrack(xywh, s, c) for (xywh, s, c) in zip(dets_second, scores_second, clss_second)]\n",
    "        else:\n",
    "            detections_second = []\n",
    "\n",
    "        r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n",
    "        dists = matching.iou_distance(r_tracked_stracks, detections_second)\n",
    "        matches, u_track, u_detection_second = matching.linear_assignment(dists, thresh=0.5)\n",
    "        for itracked, idet in matches:\n",
    "            track = r_tracked_stracks[itracked]\n",
    "            det = detections_second[idet]\n",
    "            if track.state == TrackState.Tracked:\n",
    "                track.update(det, self.frame_id)\n",
    "                activated_starcks.append(track)\n",
    "            else:\n",
    "                track.re_activate(det, self.frame_id, new_id=False)\n",
    "                refind_stracks.append(track)\n",
    "\n",
    "        for it in u_track:\n",
    "            track = r_tracked_stracks[it]\n",
    "            if not track.state == TrackState.Lost:\n",
    "                track.mark_lost()\n",
    "                lost_stracks.append(track)\n",
    "\n",
    "        detections = [detections[i] for i in u_detection]\n",
    "        dists = matching.iou_distance(unconfirmed, detections)\n",
    "        dists = matching.fuse_score(dists, detections)\n",
    "        matches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\n",
    "\n",
    "        for itracked, idet in matches:\n",
    "            unconfirmed[itracked].update(detections[idet], self.frame_id)\n",
    "            activated_starcks.append(unconfirmed[itracked])\n",
    "\n",
    "        for it in u_unconfirmed:\n",
    "            track = unconfirmed[it]\n",
    "            track.mark_removed()\n",
    "            removed_stracks.append(track)\n",
    "\n",
    "        for inew in u_detection:\n",
    "            track = detections[inew]\n",
    "            if track.score < self.det_thresh:\n",
    "                continue\n",
    "            track.activate(self.kalman_filter, self.frame_id)\n",
    "            activated_starcks.append(track)\n",
    "\n",
    "        for track in self.lost_stracks:\n",
    "            if self.frame_id - track.end_frame > self.max_time_lost:\n",
    "                track.mark_removed()\n",
    "                removed_stracks.append(track)\n",
    "\n",
    "        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\n",
    "        self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\n",
    "        self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n",
    "        self.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\n",
    "        self.lost_stracks.extend(lost_stracks)\n",
    "        self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n",
    "        self.removed_stracks.extend(removed_stracks)\n",
    "        self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n",
    "        output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n",
    "        outputs = []\n",
    "\n",
    "        for t in output_stracks:\n",
    "            output = []\n",
    "            tlwh = t.tlwh\n",
    "            tid = t.track_id\n",
    "            tlwh = np.expand_dims(tlwh, axis=0)\n",
    "            xyxy = xywh2xyxy(tlwh)\n",
    "            xyxy = np.squeeze(xyxy, axis=0)\n",
    "            output.extend(xyxy)\n",
    "            output.append(tid)\n",
    "            output.append(t.cls)\n",
    "            output.append(t.score)\n",
    "            outputs.append(output)\n",
    "\n",
    "        outputs = np.array(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def joint_stracks(tlista, tlistb):\n",
    "    exists = {}\n",
    "    res = []\n",
    "    for t in tlista:\n",
    "        exists[t.track_id] = 1\n",
    "        res.append(t)\n",
    "    for t in tlistb:\n",
    "        tid = t.track_id\n",
    "        if not exists.get(tid, 0):\n",
    "            exists[tid] = 1\n",
    "            res.append(t)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sub_stracks(tlista, tlistb):\n",
    "    stracks = {}\n",
    "    for t in tlista:\n",
    "        stracks[t.track_id] = t\n",
    "    for t in tlistb:\n",
    "        tid = t.track_id\n",
    "        if stracks.get(tid, 0):\n",
    "            del stracks[tid]\n",
    "    return list(stracks.values())\n",
    "\n",
    "\n",
    "def remove_duplicate_stracks(stracksa, stracksb):\n",
    "    pdist = matching.iou_distance(stracksa, stracksb)\n",
    "    pairs = np.where(pdist < 0.15)\n",
    "    dupa, dupb = list(), list()\n",
    "    for p, q in zip(*pairs):\n",
    "        timep = stracksa[p].frame_id - stracksa[p].start_frame\n",
    "        timeq = stracksb[q].frame_id - stracksb[q].start_frame\n",
    "        if timep > timeq:\n",
    "            dupb.append(q)\n",
    "        else:\n",
    "            dupa.append(p)\n",
    "    resa = [t for i, t in enumerate(stracksa) if not i in dupa]\n",
    "    resb = [t for i, t in enumerate(stracksb) if not i in dupb]\n",
    "    return resa, resb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f2aa4-7fad-4087-bdc5-862808bf5d7d",
   "metadata": {},
   "source": [
    "# Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2997a555-9aef-4ee6-bdf9-7c02d358540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from PIL import ImageColor\n",
    "\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee5c8808-4618-4c8c-873a-ed59b39f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, raw_point):\n",
    "        self.x = raw_point[0]\n",
    "        self.y = raw_point[1]\n",
    "\n",
    "        def to_string(self): '(' + str(self.x) + ', ' + str(self.y) + ')'\n",
    "\n",
    "        def to_dict(self): return {'x': self.x, 'y': self.y}\n",
    "\n",
    "\n",
    "class Box:\n",
    "    def __init__(self, class_name, confidence, raw_corner_points, color, track_id=None):\n",
    "        self.class_name = class_name\n",
    "        self.confidence = confidence\n",
    "        self.raw_corner_points = raw_corner_points\n",
    "        self.top_left_point = Point(raw_corner_points[0])\n",
    "        self.bottom_right_point = Point(raw_corner_points[1])\n",
    "        self.width = self.bottom_right_point.x - self.top_left_point.x\n",
    "        self.height = self.bottom_right_point.y - self.top_left_point.y\n",
    "        self.color = color\n",
    "        self.track_id = track_id\n",
    "\n",
    "    def to_dict(self):\n",
    "        box = OrderedDict([\n",
    "            ('class', self.class_name),\n",
    "            ('confidence', self.confidence),\n",
    "            ('x', self.top_left_point.x),\n",
    "            ('y', self.top_left_point.y),\n",
    "            ('width', self.width),\n",
    "            ('height', self.height),\n",
    "            ('color', self.color)\n",
    "        ])\n",
    "        if self.track_id is not None:\n",
    "            box['id'] = self.track_id\n",
    "        return box\n",
    "\n",
    "\n",
    "class Detections:\n",
    "    def __init__(self, raw_detection, classes, tracking=False):\n",
    "        self.__raw_detection = raw_detection\n",
    "        self.__classes = classes\n",
    "        self.__boxes = []\n",
    "        self.__tracking = tracking\n",
    "        self.__point1_index = 0\n",
    "        self.__point2_index = 1\n",
    "        self.__point3_index = 2\n",
    "        self.__point4_index = 3\n",
    "        self.__tracking_index = 4\n",
    "        self.__class_index = 5 if tracking else 5\n",
    "        self.__confidence_index = 6 if tracking else 4\n",
    "        self.__extract_boxes()\n",
    "\n",
    "    def __extract_boxes(self):\n",
    "        for raw_box in self.__raw_detection:\n",
    "            track_id = None\n",
    "            if self.__tracking:\n",
    "                track_id = int(raw_box[self.__tracking_index])\n",
    "            class_id = int(raw_box[self.__class_index])\n",
    "            raw_corner_points = (int(raw_box[self.__point1_index]), int(raw_box[self.__point2_index])), (\n",
    "                int(raw_box[self.__point3_index]), int(raw_box[self.__point4_index]))\n",
    "            confidence = raw_box[self.__confidence_index]\n",
    "            dataset_class = self.__classes[class_id]\n",
    "            class_name = dataset_class['name']\n",
    "            class_color = dataset_class['color']\n",
    "            box = Box(class_name, confidence, raw_corner_points, class_color, track_id=track_id)\n",
    "            self.__boxes.append(box)\n",
    "\n",
    "    def get_boxes(self):\n",
    "        return self.__boxes\n",
    "\n",
    "    def to_dict(self):\n",
    "        boxes = []\n",
    "        for box in self.__boxes:\n",
    "            boxes.append(box.to_dict())\n",
    "        return boxes\n",
    "\n",
    "    def to_json(self):\n",
    "        boxes = self.to_dict()\n",
    "        return json.dumps(boxes, indent=4)\n",
    "\n",
    "\n",
    "def plot_box(image, top_left_point, bottom_right_point, width, height, label, color=(210, 240, 0), padding=6,\n",
    "             font_scale=0.35):\n",
    "    label = label.upper()\n",
    "\n",
    "    cv2.rectangle(image, (top_left_point['x'] - 1, top_left_point['y']),\n",
    "                  (bottom_right_point['x'], bottom_right_point['y']), color, thickness=2, lineType=cv2.LINE_AA)\n",
    "    res_scale = (image.shape[0] + image.shape[1]) / 1600\n",
    "    font_scale = font_scale * res_scale\n",
    "    font_width, font_height = 0, 0\n",
    "    font_face = cv2.FONT_HERSHEY_DUPLEX\n",
    "    text_size = cv2.getTextSize(label, font_face, fontScale=font_scale, thickness=1)[0]\n",
    "\n",
    "    if text_size[0] > font_width:\n",
    "        font_width = text_size[0]\n",
    "    if text_size[1] > font_height:\n",
    "        font_height = text_size[1]\n",
    "    if top_left_point['x'] - 1 < 0:\n",
    "        top_left_point['x'] = 1\n",
    "    if top_left_point['x'] + font_width + padding * 2 > image.shape[1]:\n",
    "        top_left_point['x'] = image.shape[1] - font_width - padding * 2\n",
    "    if top_left_point['y'] - font_height - padding * 2 < 0:\n",
    "        top_left_point['y'] = font_height + padding * 2\n",
    "\n",
    "    p3 = top_left_point['x'] + font_width + padding * 2, top_left_point['y'] - font_height - padding * 2\n",
    "    cv2.rectangle(image, (top_left_point['x'] - 2, top_left_point['y']), p3, color, -1, lineType=cv2.LINE_AA)\n",
    "    x = top_left_point['x'] + padding\n",
    "    y = top_left_point['y'] - padding\n",
    "    cv2.putText(image, label, (x, y), font_face, font_scale, [0, 0, 0], thickness=1, lineType=cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def draw(image, detections):\n",
    "    image_copy = image.copy()\n",
    "    for box in detections:\n",
    "        class_name = box['class']\n",
    "        conf = box['confidence']\n",
    "        text = ''\n",
    "        if 'text' in box:\n",
    "            text = box['text']\n",
    "            if len(text) > 50:\n",
    "                text = text[:50] + ' ...'\n",
    "        label = (str(box['id']) + '. ' if 'id' in box else '') + class_name + ' ' + str(int(conf * 100)) + '%' + (\n",
    "            (' | ' + text) if ('text' in box and len(box['text']) > 0 and not box['text'].isspace()) else '')\n",
    "        width = box['width']\n",
    "        height = box['height']\n",
    "        color = box['color']\n",
    "\n",
    "        if isinstance(color, str):\n",
    "            color = ImageColor.getrgb(color)\n",
    "            color = (color[2], color[1], color[0])\n",
    "\n",
    "        top_left_point = {'x': box['x'], 'y': box['y']}\n",
    "        bottom_right_point = {'x': box['x'] + width, 'y': box['y'] + height}\n",
    "        image_copy = plot_box(image_copy, top_left_point, bottom_right_point, width, height, label, color=color)\n",
    "    return image_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e953fa-6450-4844-997b-4e9312026783",
   "metadata": {},
   "source": [
    "# Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4594b454-bd13-4a69-8ad6-f3512fc9c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.models.experimental import attempt_load\n",
    "from yolov7.utils.datasets import letterbox\n",
    "from yolov7.utils.general import check_img_size, non_max_suppression, scale_coords\n",
    "\n",
    "from yolov7.utils.torch_utils import select_device\n",
    "from detections import Detections\n",
    "from ByteTrack.yolox.tracker.byte_tracker import BYTETracker\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BYTETrackerArgs:\n",
    "    track_thresh: float = 0.25\n",
    "    track_buffer: int = 30\n",
    "    match_thresh: float = 0.8\n",
    "    aspect_ratio_thresh: float = 3.0\n",
    "    min_box_area: float = 1.0\n",
    "    mot20: bool = False\n",
    "\n",
    "\n",
    "class YOLOv7:\n",
    "\n",
    "    def __init__(self, conf_thres=0.25, iou_thres=0.45, img_size=640):\n",
    "        self.settings = {\n",
    "            'conf_thres': conf_thres,\n",
    "            'iou_thres': iou_thres,\n",
    "            'img_size': img_size,\n",
    "        }\n",
    "        self.tracker = BYTETracker(BYTETrackerArgs)\n",
    "        self.text_recognizer = None\n",
    "\n",
    "    def load(self, weights_path, classes, device='cpu'):\n",
    "        with torch.no_grad():\n",
    "            self.device = select_device(device)\n",
    "            self.model = attempt_load(weights_path, map_location=self.device)\n",
    "\n",
    "            if device != 'cpu':\n",
    "                self.model.half()\n",
    "                self.model.to(self.device).eval()\n",
    "\n",
    "            stride = int(self.model.stride.max())\n",
    "            self.imgsz = check_img_size(self.settings['img_size'], s=stride)\n",
    "            self.classes = yaml.load(open(file=classes, mode=\"r\", encoding=\"utf8\"), Loader=yaml.SafeLoader)['classes']\n",
    "\n",
    "    def unload(self):\n",
    "        if self.device.type != 'cpu':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def set(self, **config):\n",
    "        for key in config.keys():\n",
    "            if key in self.settings.keys():\n",
    "                self.settings[key] = config[key]\n",
    "            else:\n",
    "                raise Exception(f'{key} is not a valid inference setting')\n",
    "\n",
    "    def __parse_image(self, img):\n",
    "        im0 = img.copy()\n",
    "        img = letterbox(im0, self.imgsz, auto=self.imgsz != 1280)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(self.device)\n",
    "        img = img.half() if self.device.type != 'cpu' else img.float()\n",
    "        img /= 255.0\n",
    "\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        return im0, img\n",
    "\n",
    "    def detect(self, img, track=False):\n",
    "        with torch.no_grad():\n",
    "            im0, img = self.__parse_image(img)\n",
    "            pred = self.model(img)[0]\n",
    "            pred = non_max_suppression(pred, self.settings['conf_thres'], self.settings['iou_thres'])\n",
    "            raw_detection = np.empty((0, 6), float)\n",
    "\n",
    "            for det in pred:\n",
    "                if len(det) > 0:\n",
    "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    for *xyxy, conf, cls in reversed(det):\n",
    "                        raw_detection = np.concatenate((raw_detection, [\n",
    "                            [int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3]), round(float(conf), 2), int(cls)]]))\n",
    "\n",
    "            if track:\n",
    "                raw_detection = self.tracker.update(raw_detection)\n",
    "\n",
    "            detections = Detections(raw_detection, self.classes, tracking=track).to_dict()\n",
    "\n",
    "            return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2f419-ef81-485c-a9d8-b77b99cdedc4",
   "metadata": {},
   "source": [
    "# Track Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d6608-f34e-4b76-84da-28cef2eb53ac",
   "metadata": {},
   "source": [
    "To test our model prototype, a trimmed video will be employed to cut time for our model detections and tracking. The VIRAT_S_000001 video will be trimmed and used for our testing purposes.\n",
    "\n",
    "**Original Video:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000b8787-380f-40af-9f36-328565509848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"VIRAT Ground Dataset/videos_original/VIRAT_S_000001.MP4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('VIRAT Ground Dataset/videos_original/VIRAT_S_000001.MP4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7034d-6e0b-478d-a301-ae353ba9ebad",
   "metadata": {},
   "source": [
    "**Trimmed Video:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92913832-36cd-4506-b74e-40a9caf99229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"VIRAT_S_000001_Trimmed.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('VIRAT_S_000001_Trimmed.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b1307-7c69-4bd9-ae20-1effbcd4b3ae",
   "metadata": {},
   "source": [
    "Now that we have our trimmed video, we can proceed with testing our tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58aa96f0-2f43-4d6f-afd3-689714426d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = f'{HOME_DIR}/yolov7/yolov7.pt'\n",
    "CLASSES_PATH = f'{HOME_DIR}/track_classes.yaml'\n",
    "VIDEO_PATH = f'{HOME_DIR}/VIRAT_S_000001_Trimmed.mp4'\n",
    "OUTPUT_PATH = f'{HOME_DIR}/VIRAT_S_000001_Trimmed_Tracked.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34c2c42-5c65-4700-98b5-35c63ece187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Tracking Objects...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 7500/7500 [1:33:31<00:00,  1.34 frames/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "sys.path.insert(0, './yolov7')\n",
    "\n",
    "yolov7 = YOLOv7()\n",
    "yolov7.load(weights_path=WEIGHTS_PATH, classes=CLASSES_PATH, device='cpu')\n",
    "\n",
    "video = cv2.VideoCapture(VIDEO_PATH)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "frames_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "output = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "if video.isOpened() == False:\n",
    "    print('Error Opening Video!')\n",
    "    \n",
    "print('Tracking Objects...\\n')\n",
    "pbar = tqdm(total=frames_count, unit=' frames', dynamic_ncols=True, position=0, leave=True)\n",
    "\n",
    "try:\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            detections = yolov7.detect(frame, track=True)\n",
    "            detected_frame = draw(frame, detections)\n",
    "            output.write(detected_frame)\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "pbar.close()\n",
    "video.release()\n",
    "output.release()\n",
    "yolov7.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521afc83-bef3-40bc-9b4c-35e202c15f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"VIRAT_S_000001_Trimmed_Tracked.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video('VIRAT_S_000001_Trimmed_Tracked.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf0c38-387f-4276-b734-ad0f9ecc04dd",
   "metadata": {},
   "source": [
    "As we can see, the tracking was a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7e866-4f1e-4904-95a8-3476b31bec2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Capstone Project)",
   "language": "python",
   "name": "capstone-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
